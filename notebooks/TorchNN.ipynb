{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee747a55",
   "metadata": {},
   "source": [
    "# `torch.nn` & `torch.nn.functional`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca0bdf5",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdceaf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481be74",
   "metadata": {},
   "source": [
    "## Convolution Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50647298",
   "metadata": {},
   "source": [
    "### Convnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87d312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnn.Conv3d(\\n    in_channels=Cin,\\n    out_channels=Cout,\\n    kernel_size=K,\\n    stride=S=1,\\n    padding=P=0,\\n    dilation=D=1\\n):  (N?, Cin, Din, Hin, Win)\\n->  (N?, Cout, Dout, Hout, Wout),\\n    Dout = (Din + 2 * P - D * (K - 1) - 1) // S + 1\\n    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\\n    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "nn.Conv1d(\n",
    "    in_channels=Cin,\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Lin)\n",
    "->  (N?, Cout, Lout),\n",
    "    Lout = (Lin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "nn.Conv2d(\n",
    "    in_channels=Cin,\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Hin, Win)\n",
    "->  (N?, Cout, Hout, Wout),\n",
    "    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "nn.Conv3d(\n",
    "    in_channels=Cin,\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Din, Hin, Win)\n",
    "->  (N?, Cout, Dout, Hout, Wout),\n",
    "    Dout = (Din + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4c698",
   "metadata": {},
   "source": [
    "### LazyConvnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e737b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnn.LazyConv3d(\\n    out_channels=Cout,\\n    kernel_size=K,\\n    stride=S=1,\\n    padding=P=0,\\n    dilation=D=1\\n):  (N?, Cin, Din, Hin, Win)\\n->  (N?, Cout, Dout, Hout, Wout),\\n    Dout = (Din + 2 * P - D * (K - 1) - 1) // S + 1\\n    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\\n    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "nn.LazyConv1d(\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Lin)\n",
    "->  (N?, Cout, Lout),\n",
    "    Lout = (Lin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "nn.LazyConv2d(\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Hin, Win)\n",
    "->  (N?, Cout, Hout, Wout),\n",
    "    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "nn.LazyConv3d(\n",
    "    out_channels=Cout,\n",
    "    kernel_size=K,\n",
    "    stride=S=1,\n",
    "    padding=P=0,\n",
    "    dilation=D=1\n",
    "):  (N?, Cin, Din, Hin, Win)\n",
    "->  (N?, Cout, Dout, Hout, Wout),\n",
    "    Dout = (Din + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Hout = (Hin + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "    Wout = (Win + 2 * P - D * (K - 1) - 1) // S + 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34d1276",
   "metadata": {},
   "source": [
    "### ConvTransposend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54683d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "075bf01e",
   "metadata": {},
   "source": [
    "### LazyConvTransposend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b4435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d7a7b1f",
   "metadata": {},
   "source": [
    "## Recurrent Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20945bd4",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d388f321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.RNN(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D]\n",
    "):  (*B, Hin),    (D*C, N?, Hout)\n",
    "->  (*B, D*Hout), (D*C, N?, Hout)\n",
    "\"\"\"\n",
    "\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "# https://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, nonlinearity, bias = 2, 3, 4, \"tanh\", True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional = False, False\n",
    "\n",
    "rnn = nn.RNN(\n",
    "    input_size,  # Hin, Required\n",
    "    hidden_size,  # Hout, Required\n",
    "    num_layers,  # C, default=1\n",
    "    nonlinearity,  # \"tanh\" or \"relu\", default=\"tanh\"\n",
    "    bias,  # default=True\n",
    "    batch_first=batch_first,  # default=False\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,  # default=False\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 5, 6\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h0 = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, h = rnn(x, None if h0 is None else h0)\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * hidden_size)  # (N, L, D * Hout)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * hidden_size)  # (L, N, D * Hout)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1c7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.RNNCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin),  (N?, Hout)\n",
    "->  (N?, Hout), (N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.RNNCell.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias, nonlinearity = 2, 3, True, \"tanh\"\n",
    "\n",
    "\n",
    "rnn_cell = nn.RNNCell(\n",
    "    input_size,  # Hin, Required\n",
    "    hidden_size,  # Hout, Required\n",
    "    bias,  # default=True\n",
    "    nonlinearity,  # \"tanh\" or \"relu\", default=\"tanh\"\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "f = {\"tanh\": F.tanh, \"relu\": F.relu}[nonlinearity]\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h = rnn_cell(x, None if h0 is None else h0)\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "assert torch.allclose(\n",
    "    h,\n",
    "    f(\n",
    "        F.linear(x, rnn_cell.weight_ih, rnn_cell.bias_ih)\n",
    "        + F.linear(h0, rnn_cell.weight_hh, rnn_cell.bias_hh)\n",
    "    ),\n",
    "    atol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4815dce",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f21806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.LSTM(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D],\n",
    "    proj_size=P=P if P > 0 else Hout\n",
    "):  (*B, Hin), ((D*C, N?, P), (D*C, N?, Hout))\n",
    "->  (*B, D*P), ((D*C, N?, P), (D*C, N?, Hout))\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "# https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, bias = 2, 3, 1, True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional, proj_size = False, True, 1\n",
    "\n",
    "assert proj_size < hidden_size, \"proj_size has to be smaller than hidden_size\"\n",
    "\n",
    "lstm = nn.LSTM(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    num_layers,  # C\n",
    "    bias=bias,\n",
    "    batch_first=batch_first,\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,\n",
    "    proj_size=proj_size,  # default=0\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "P = proj_size if proj_size > 0 else hidden_size\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 4, 5\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h = torch.randn(D * num_layers, batch_size, P)  # (D * C, N, P)\n",
    "c = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, (h, c) = lstm(x, None if h is None or c is None else (h, c))\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * P)  # (N, L, D * P)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * P)  # (L, N, D * P)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, P)  # (D * C, N, P)\n",
    "assert c.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8973ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.LSTMCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin), ((N?, P), (N?, Hout))\n",
    "->  (N?, P),   ((N?, P), (N?, Hout))\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "# https://yb.tencent.com/s/4OGnkvsVzqDH\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias = 2, 3, True\n",
    "\n",
    "lstm_cell = nn.LSTMCell(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "c0 = torch.randn(batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h, c = lstm_cell(x, None if h0 is None or c0 is None else (h0, c0))\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "assert c.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "gates = F.linear(x, lstm_cell.weight_ih, lstm_cell.bias_ih) + F.linear(\n",
    "    h0, lstm_cell.weight_hh, lstm_cell.bias_hh\n",
    ")\n",
    "i, f, g, o = gates.chunk(4, dim=1)  # Split into input, forget, gate and output\n",
    "i = F.sigmoid(i)\n",
    "f = F.sigmoid(f)\n",
    "g = F.tanh(g)\n",
    "o = F.sigmoid(o)\n",
    "\n",
    "c1 = f * c0 + i * g  # Update cell state\n",
    "h1 = o * F.tanh(c)  # Update hidden state\n",
    "\n",
    "assert torch.allclose(h, h1, atol=1e-6)\n",
    "assert torch.allclose(c, c1, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d330e1d",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb75245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.GRU(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "    num_layers=C=1,\n",
    "    batch_first={(L, N?): False, (N?, L): True}[B],\n",
    "    bidirectional={1: False, 2: True}[D]\n",
    "):  (*B, Hin),    (D*C, N?, Hout)\n",
    "->  (*B, D*Hout), (D*C, N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, num_layers, bias = 2, 3, 1, True\n",
    "\n",
    "\"\"\"2. Keyword Arguments\"\"\"\n",
    "batch_first, bidirectional = False, True\n",
    "\n",
    "gru = nn.GRU(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    num_layers,  # C\n",
    "    bias,\n",
    "    batch_first=batch_first,\n",
    "    dropout=0.0,\n",
    "    bidirectional=bidirectional,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "D = 2 if bidirectional else 1\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size, seq_len = 4, 5\n",
    "\n",
    "if batch_first:\n",
    "    x = torch.randn(batch_size, seq_len, input_size)  # (N, L, Hin)\n",
    "else:\n",
    "    x = torch.randn(seq_len, batch_size, input_size)  # (L, N, Hin)\n",
    "\n",
    "h = torch.randn(D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "y, h = gru(x, None if h is None else h)\n",
    "\n",
    "if batch_first:\n",
    "    assert y.shape == (batch_size, seq_len, D * hidden_size)  # (N, L, D * Hout)\n",
    "else:\n",
    "    assert y.shape == (seq_len, batch_size, D * hidden_size)  # (L, N, D * Hout)\n",
    "\n",
    "assert h.shape == (D * num_layers, batch_size, hidden_size)  # (D * C, N, Hout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96cfdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.GRUCell(\n",
    "    input_size=Hin,\n",
    "    hidden_size=Hout,\n",
    "):  (N?, Hin),  (N?, Hout)\n",
    "->  (N?, Hout), (N?, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.GRUCell.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "input_size, hidden_size, bias = 2, 3, True\n",
    "\n",
    "gru_cell = nn.GRUCell(\n",
    "    input_size,  # Hin\n",
    "    hidden_size,  # Hout\n",
    "    bias=bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "\"\"\"1. Inputs\"\"\"\n",
    "batch_size = 4\n",
    "\n",
    "x0 = torch.randn(batch_size, input_size)  # (N, Hin)\n",
    "\n",
    "h0 = torch.randn(batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "\n",
    "\"\"\"2. Outputs\"\"\"\n",
    "h = gru_cell(x0, None if h0 is None else h0)\n",
    "\n",
    "assert h.shape == (batch_size, hidden_size)  # (N, Hout)\n",
    "\n",
    "x_gates = F.linear(x0, gru_cell.weight_ih, gru_cell.bias_ih).chunk(3, dim=1)\n",
    "h_gates = F.linear(h0, gru_cell.weight_hh, gru_cell.bias_hh).chunk(3, dim=1)\n",
    "r = F.sigmoid(x_gates[0] + h_gates[0])  # Reset gate\n",
    "z = F.sigmoid(x_gates[1] + h_gates[1])  # Update gate\n",
    "n = F.tanh(x_gates[2] + r * h_gates[2])  # New gate\n",
    "h1 = (torch.ones_like(z) - z) * n + z * h0  # Update hidden state\n",
    "\n",
    "assert torch.allclose(h, h1, atol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d88b6",
   "metadata": {},
   "source": [
    "## Linear Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94d1f8",
   "metadata": {},
   "source": [
    "### Identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7142f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.Identity(): (*) -> (*)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.Identity.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "identity = nn.Identity()\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "x = torch.randn(2, 3)\n",
    "y = identity(x)\n",
    "\n",
    "assert y.shape == x.shape\n",
    "assert torch.equal(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989bd47",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3a60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.Linear(\n",
    "    in_features=Hin,\n",
    "    out_features=Hout,\n",
    "):  (*, Hin)\n",
    "->  (*, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "in_features, out_features, bias = 2, 3, True\n",
    "\n",
    "linear = nn.Linear(\n",
    "    in_features,  # Hin, Required\n",
    "    out_features,  # Hout, Required\n",
    "    bias,  # default=True\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "batch_sizes = (2, 3, 4)\n",
    "\n",
    "x = torch.randn(*batch_sizes, in_features)  # (N, Hin)\n",
    "y = linear(x)\n",
    "\n",
    "assert y.shape == (*batch_sizes, out_features)  # (N, Hout)\n",
    "assert torch.allclose(y, F.linear(x, linear.weight, linear.bias))\n",
    "assert torch.allclose(\n",
    "    F.linear(x, linear.weight, linear.bias),\n",
    "    x @ linear.weight.t() + (linear.bias if bias else 0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6010f8c9",
   "metadata": {},
   "source": [
    "### Bilinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae3ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.Bilinear(\n",
    "    in1_features=Hin1,\n",
    "    in2_features=Hin2,\n",
    "    out_features=Hout,\n",
    "):  (*, Hin1), (*, Hin2)\n",
    "->  (*, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.Bilinear.html\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "in1_features, in2_features, out_features, bias = 2, 3, 4, True\n",
    "\n",
    "bilinear = nn.Bilinear(\n",
    "    in1_features,  # Hin1, Required\n",
    "    in2_features,  # Hin2, Required\n",
    "    out_features,  # Hout, Required\n",
    "    bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "batch_sizes = (2, 3, 4)\n",
    "\n",
    "x1 = torch.randn(*batch_sizes, in1_features)  # (N, Hin1)\n",
    "x2 = torch.randn(*batch_sizes, in2_features)  # (N, Hin2)\n",
    "y = bilinear(x1, x2)\n",
    "\n",
    "assert y.shape == (*batch_sizes, out_features)  # (N, Hout)\n",
    "assert torch.allclose(\n",
    "    y,\n",
    "    F.bilinear(x1, x2, bilinear.weight, bilinear.bias),\n",
    ")\n",
    "assert torch.allclose(\n",
    "    y,\n",
    "    torch.einsum(\"...i,kij,...j->...k\", x1, bilinear.weight, x2)\n",
    "    + (bilinear.bias if bias else 0),\n",
    "    atol=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b92d4ce",
   "metadata": {},
   "source": [
    "### LazyLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ddc3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "nn.LazyLinear(\n",
    "    out_features=Hout\n",
    "):  (*, Hin)\n",
    "->  (*, Hout)\n",
    "\"\"\"\n",
    "# https://docs.pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html\n",
    "\n",
    "\"\"\"Define Layer\"\"\"\n",
    "\"\"\"1. Position Arguments\"\"\"\n",
    "out_features, bias = 2, True\n",
    "\n",
    "lazy_linear = nn.LazyLinear(\n",
    "    out_features,  # Hout\n",
    "    bias,\n",
    "    device=None,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "\"\"\"Forward Pass\"\"\"\n",
    "\n",
    "batch_size, in_features = (3, 4), 5\n",
    "\n",
    "x = torch.randn(*batch_size, in_features)  # (N, Hin)\n",
    "y = lazy_linear(x)\n",
    "\n",
    "assert y.shape == (*batch_size, out_features)  # (N, Hout)\n",
    "assert torch.allclose(y, F.linear(x, lazy_linear.weight, lazy_linear.bias), atol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
